{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "- This notebook is for visualization of acoustic change over time.\n",
    "- Example data are from the TransVoices project: two YouTube vloggers who are transwomen, whose videos over a period of seven years have been transcribed as TextGrids.\n",
    "- The TextGrids were force aligned using a multi-tier version of FAVE, and formant measurements were made using ifcformant (thanks to Ron Sprouse for those scripts).\n",
    "- The files are ''[name]_multi-align.TextGrid'' in the folder \"multi-align\" and ''[name].ifc'' in the folder \"ifc_files\".\n",
    "- The .ifc and .TextGrid files are merged, the normalized by speaker and by local speech rate.\n",
    "test link "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from audiolabel import read_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The relevant files (ifc files and textgrids) are stored here; set up so files can be read from this object\n",
    "wdifc = './ifc_files/'\n",
    "wdtg = './multi_align/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a df with a column of .ifc filenames using regular expressions\n",
    "# The tg files are named (e.g.,) JV_013110_Title.ifc/tg, so split by underscore to get speaker, date, and video title.\n",
    "ifcdf = pd.DataFrame(os.listdir(wdifc), columns=['ifcname'])\n",
    "code = ifcdf.ifcname.str.extract(r'^(?P<speaker>.+)_(?P<date>\\d+)_(?P<title>.+)\\.ifc$', expand=True)\n",
    "ifcdf = pd.concat([ifcdf, code], axis=1)\n",
    "# Create a df with a column of .tg filenames using regular expressions\n",
    "tgdf = pd.DataFrame(os.listdir(wdtg), columns=['tgname'])\n",
    "codetg = tgdf.tgname.str.extract(r'^(?P<speaker>.+)_(?P<date>\\d+)_(?P<title>.+)\\.multi_align\\.TextGrid$', expand=True)\n",
    "tgdf = pd.concat([tgdf, codetg], axis=1)\n",
    "# tgdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge .ifc list dataframe and .textgrid list dataframe, overlapping the columns in 'code'\n",
    "code = ['speaker','date','title']\n",
    "matchdf = pd.merge(ifcdf, tgdf, on=code, how='inner')\n",
    "# matchdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile a dataframe from all of the .textgrid files using audiolabel\n",
    "# This extracts all the phones and words from the multi-aligned TextGrids\n",
    "tgnames = [os.path.join(wdtg, tgname) for tgname in matchdf.tgname]\n",
    "[phonedf, worddf] = read_label(tgnames, 'praat', addcols=['barename', 'fidx'], tiers=['phone','word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57578"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create two new columns, 'vowel' and 'stress', based on regular expressions in 'phone' column\n",
    "stress = phonedf.phone.str.extract(r'^(?P<vowel>.+)(?P<stress>\\d+)$', expand=True)\n",
    "# Concatenate the stress column to phonedf\n",
    "phonedf = pd.concat([phonedf, stress], axis=1)\n",
    "phonedf['stress'].count() # this is how many syllables are in the entire phonedf\n",
    "# phonedf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get local speech rate from textgrid\n",
    "Thanks to Geoff Bacon for this section. First function gets local speech rate in syllables, defined as stressed vowels per 20 TextGrid rows. Second and third functions get local speech rate in syllables, defined as stressed vowels per 30 seconds in TextGrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_rolling_stress_count(df, window_size):\n",
    "    \"\"\"\n",
    "    Returns `df` with an extra column of the moving count of the stress column.\n",
    "    `df` must have a column called `stress` and `fname` (identifying each video, for groupby)\n",
    "    Counts all values in `stress` as syllables in `binary_stress`.\n",
    "    Uses `.rolling` to count `binary_stress` values in a window size of X rows.\n",
    "    Fills in edge cases (begininng and end of window) using `bfill` and `ffill`.\n",
    "    \"\"\"\n",
    "    df['binary_stress'] = df['stress'].notnull()\n",
    "    df['binary_stress'].replace({False: 0, True: 1}, inplace=True)\n",
    "    grouped = df.groupby('fname')\n",
    "    rolling_count = grouped.rolling(window_size, center=True).sum()['binary_stress']\n",
    "    rolling_count = rolling_count.groupby(level='fname').fillna(method='bfill')\n",
    "    rolling_count = rolling_count.groupby(level='fname').fillna(method='ffill')\n",
    "    multiindexed_df = df.set_index('fname', append=True).swaplevel()\n",
    "    renamed_rolling_count = rolling_count.rename('rolling_stress_count').to_frame()\n",
    "    merged = pd.merge(multiindexed_df, renamed_rolling_count, left_index=True, right_index=True)\n",
    "    df_with_rolling_count = merged.reset_index().drop('level_1', axis=1)\n",
    "    return df_with_rolling_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to phonedf with a window size of 20 rows\n",
    "df = make_rolling_stress_count(phonedf, 20)\n",
    "phonedf = df\n",
    "#phonedf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def process(dataframe, offset):\n",
    "    \"\"\"\n",
    "    Changes `t1` in dataframe to seconds units, counts up all values in `binary_stress`\n",
    "    \"\"\"\n",
    "    dataframe['t1_as_datetime'] = pd.to_datetime(dataframe['t1'], unit='s')\n",
    "    dataframe['rolling_count'] = dataframe.rolling('30s', on='t1_as_datetime').sum()['binary_stress']\n",
    "    start = dataframe['t1_as_datetime'].iloc[0]\n",
    "    dataframe['offset'] = dataframe['t1_as_datetime'] - start\n",
    "    dataframe['beginning'] = dataframe['offset'] < offset\n",
    "    dataframe.loc[dataframe['beginning'], 'rolling_count'] = np.NaN\n",
    "    dataframe['rolling_count'].fillna(method='bfill', inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_temporal_rolling_stress_count(df, time_in_seconds):\n",
    "    \"\"\"\n",
    "    Returns `df` with an extra column of the moving count of the stress column of length `time_in_seconds`.\n",
    "    `df` must have a column called `stress` and `fname`.\n",
    "    Uses `.rolling` to count `binary_stress` values per window of 30 seconds.\n",
    "    \"\"\"\n",
    "    offset = timedelta(seconds=time_in_seconds)\n",
    "    \n",
    "    fnames = phonedf['fname'].unique()\n",
    "    result = []\n",
    "    for fname in fnames:\n",
    "        tmp = phonedf[phonedf['fname'] == fname]\n",
    "        tmp = process(tmp, offset)\n",
    "        result.append(tmp)\n",
    "    df = pd.concat(result, ignore_index=True)\n",
    "    return df.drop(['offset', 't1_as_datetime', 'beginning'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply function to phonedf with a window size of 30 seconds\n",
    "df = make_temporal_rolling_stress_count(phonedf, 30)\n",
    "phonedf = df\n",
    "#phonedf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge phone and word dataframes to create full textgrid dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add 't1_w' column to word textgrids, which is copy of 't1'\n",
    "worddf = worddf.assign(t1_w = worddf.t1)\n",
    "\n",
    "# Define a function that merges phone and word textgrids\n",
    "def mergepw(p, w):\n",
    "    f = p['fidx'].values[0] # find fidx value of argument 'p', assign it to 'f'\n",
    "    locw = w.loc[w.fidx==f,['t1','t1_w','t2','word']] # select the words that have fidx value 'f'\n",
    "    return pd.merge_asof(p, locw, on='t1', suffixes=['_p','_w']) # create a suffix for the duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the function to merge phone and word textgrids and create the full textgrid dataframe\n",
    "fulltgdf = phonedf.groupby('fidx').apply(mergepw, w=worddf)\n",
    "fulltgdf = fulltgdf.rename(columns={'t1':'t1_p'}) # rename 't1' column to 't1_p'\n",
    "fulltgdf = fulltgdf.assign(barename = fulltgdf.barename.str.replace('.multi_align','')) # take out the extension \n",
    "# fulltgdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge fulltgdf with ifc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Columns to read from .ifc files. ###\n",
    "usecols = ['sec','rms','f0', 'f1', 'f2', 'f3', 'f4']\n",
    "# Compile a dataframe from all of the .ifc files.\n",
    "dfs = []\n",
    "for ifcname in matchdf.ifcname:\n",
    "    df = pd.read_table(os.path.join(wdifc, ifcname), usecols=usecols)\n",
    "    df = df.assign(ifcname=ifcname)\n",
    "    dfs.append(df)\n",
    "fullifcdf = pd.concat(dfs)\n",
    "fullifcdf = fullifcdf.assign(barename = fullifcdf.ifcname.str.replace('.ifc','')) # take out the extension\n",
    "# fullifcdf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read column from each .ifc file and merge on t1 with corresponding .textgrid file\n",
    "def mergeit(i, pw):\n",
    "    b = i['barename'].values[0] # find barename value of argument 'i', assign it to 'b'\n",
    "    locb = pw.loc[pw.barename==b,:] # select the words that have barename value 'b'\n",
    "    return pd.merge_asof(i, locb, left_on='sec', right_on='t1_p')\n",
    "\n",
    "full_df = fullifcdf.groupby('barename').apply(mergeit, pw=fulltgdf)\n",
    "# full_df.head() # initial check of full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "dropcols = ['barename_x','barename_y','fidx','fname']\n",
    "full_df = full_df.drop(dropcols, axis=1)\n",
    "\n",
    "# Add columns for speaker, date, and time, taken from 'ifcname' values\n",
    "sdt = full_df.ifcname.str.extract(r'^(?P<speaker>.+)_(?P<date>\\d+)_(?P<title>.+)\\.ifc$', expand=True)\n",
    "full_df = pd.concat([full_df, sdt], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# replace all empty 'phone' cells with NaN, then drop those rows\n",
    "full_df['phone'].replace('', np.nan, inplace=True)\n",
    "full_df.dropna(subset=['phone'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add ipa column.\n",
    "ph2ipa = pd.read_table('arpabet2ipa.txt', names=('phone','ipa')) # Read in conversion table\n",
    "full_df = full_df.merge(ph2ipa, how='left', on='phone') # Merge conversion table in with vowels\n",
    "full_df.dropna(subset=['ipa'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate duration and local speech rate normalized duration of each phone\n",
    "full_df['duration'] = full_df['t2_p'] - full_df['t1_p']\n",
    "full_df['norm_duration'] = full_df['duration']/full_df['rolling_count']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert dates to useable formats\n",
    "full_df['olddatetime'] = pd.to_datetime(full_df['date'], format='%m%d%y', errors='coerce') # datetime format\n",
    "full_df['datetime'] = full_df.olddatetime.astype(str).str.strip() # convert to string\n",
    "# full_df.head() # final check of full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Bin videos  by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create up to 31 bins of 20 seconds each for each video, grouping by 'datetime'\n",
    "binseq = full_df.groupby('datetime').apply(\n",
    "            lambda y: np.array([0,20,40,60,80,100,120,140,160,180,200,220,240,260,280,300,\n",
    "                  320,340,360,380,400,420,440,460,480,500,520,540,560,580,600,620]\n",
    "                               + y.t1_p.min() - 0.01)) # make bins of 20 secs each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create  a dataframe that cuts full_df into 30 bins for each video, grouping by 'datetime'\n",
    "ebins = pd.DataFrame(full_df.groupby('datetime').apply(\n",
    "    lambda q: pd.cut(q['t1_p'], bins=30, labels=False)))\n",
    "ebins = ebins.rename(columns={'t1_p':'binidx'}) #ebins should have 30 bins per video\n",
    "\n",
    "# Create a dataframe that cuts full_df into 20sec bins for each video, grouping by 'datetime'\n",
    "bins = pd.DataFrame(full_df.groupby('datetime').apply(\n",
    "    lambda r: pd.cut(r['t1_p'], bins=binseq.loc[r.name], labels=False)))\n",
    "bins = bins.rename(columns={'t1_p':'binidx'}) #binseq should have variable bins per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = bins.reset_index(level=0) # Remove 'dt' from multi-index and make it a regular column\n",
    "ebins = ebins.reset_index(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_df = full_df.assign(bins=bins.binidx) # Assign values in bins/ebins to the relevant columns in full_df\n",
    "full_df = full_df.assign(ebins=ebins.binidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec</th>\n",
       "      <th>rms</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f0</th>\n",
       "      <th>ifcname</th>\n",
       "      <th>t1_p</th>\n",
       "      <th>t2_p</th>\n",
       "      <th>...</th>\n",
       "      <th>speaker</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>ipa</th>\n",
       "      <th>duration</th>\n",
       "      <th>norm_duration</th>\n",
       "      <th>olddatetime</th>\n",
       "      <th>datetime</th>\n",
       "      <th>bins</th>\n",
       "      <th>ebins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1507125</th>\n",
       "      <td>436.425</td>\n",
       "      <td>2370.6</td>\n",
       "      <td>840.0</td>\n",
       "      <td>1451.4</td>\n",
       "      <td>3140.0</td>\n",
       "      <td>3924.0</td>\n",
       "      <td>160.9</td>\n",
       "      <td>JV_120517_AdamsApple.ifc</td>\n",
       "      <td>436.2838</td>\n",
       "      <td>436.4734</td>\n",
       "      <td>...</td>\n",
       "      <td>JV</td>\n",
       "      <td>120517</td>\n",
       "      <td>AdamsApple</td>\n",
       "      <td>ɑ</td>\n",
       "      <td>0.1896</td>\n",
       "      <td>0.125563</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507126</th>\n",
       "      <td>436.435</td>\n",
       "      <td>2041.7</td>\n",
       "      <td>962.4</td>\n",
       "      <td>1450.8</td>\n",
       "      <td>3090.6</td>\n",
       "      <td>3914.4</td>\n",
       "      <td>161.0</td>\n",
       "      <td>JV_120517_AdamsApple.ifc</td>\n",
       "      <td>436.2838</td>\n",
       "      <td>436.4734</td>\n",
       "      <td>...</td>\n",
       "      <td>JV</td>\n",
       "      <td>120517</td>\n",
       "      <td>AdamsApple</td>\n",
       "      <td>ɑ</td>\n",
       "      <td>0.1896</td>\n",
       "      <td>0.125563</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507127</th>\n",
       "      <td>436.445</td>\n",
       "      <td>1350.4</td>\n",
       "      <td>1043.9</td>\n",
       "      <td>1466.7</td>\n",
       "      <td>3053.3</td>\n",
       "      <td>3918.9</td>\n",
       "      <td>163.3</td>\n",
       "      <td>JV_120517_AdamsApple.ifc</td>\n",
       "      <td>436.2838</td>\n",
       "      <td>436.4734</td>\n",
       "      <td>...</td>\n",
       "      <td>JV</td>\n",
       "      <td>120517</td>\n",
       "      <td>AdamsApple</td>\n",
       "      <td>ɑ</td>\n",
       "      <td>0.1896</td>\n",
       "      <td>0.125563</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507128</th>\n",
       "      <td>436.455</td>\n",
       "      <td>957.7</td>\n",
       "      <td>1045.8</td>\n",
       "      <td>1477.5</td>\n",
       "      <td>3073.5</td>\n",
       "      <td>3881.9</td>\n",
       "      <td>164.6</td>\n",
       "      <td>JV_120517_AdamsApple.ifc</td>\n",
       "      <td>436.2838</td>\n",
       "      <td>436.4734</td>\n",
       "      <td>...</td>\n",
       "      <td>JV</td>\n",
       "      <td>120517</td>\n",
       "      <td>AdamsApple</td>\n",
       "      <td>ɑ</td>\n",
       "      <td>0.1896</td>\n",
       "      <td>0.125563</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507129</th>\n",
       "      <td>436.465</td>\n",
       "      <td>946.3</td>\n",
       "      <td>928.5</td>\n",
       "      <td>1449.1</td>\n",
       "      <td>3097.0</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>88.4</td>\n",
       "      <td>JV_120517_AdamsApple.ifc</td>\n",
       "      <td>436.2838</td>\n",
       "      <td>436.4734</td>\n",
       "      <td>...</td>\n",
       "      <td>JV</td>\n",
       "      <td>120517</td>\n",
       "      <td>AdamsApple</td>\n",
       "      <td>ɑ</td>\n",
       "      <td>0.1896</td>\n",
       "      <td>0.125563</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>2017-12-05</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sec     rms      f1      f2      f3      f4     f0  \\\n",
       "1507125  436.425  2370.6   840.0  1451.4  3140.0  3924.0  160.9   \n",
       "1507126  436.435  2041.7   962.4  1450.8  3090.6  3914.4  161.0   \n",
       "1507127  436.445  1350.4  1043.9  1466.7  3053.3  3918.9  163.3   \n",
       "1507128  436.455   957.7  1045.8  1477.5  3073.5  3881.9  164.6   \n",
       "1507129  436.465   946.3   928.5  1449.1  3097.0  3847.0   88.4   \n",
       "\n",
       "                          ifcname      t1_p      t2_p  ...  speaker    date  \\\n",
       "1507125  JV_120517_AdamsApple.ifc  436.2838  436.4734  ...       JV  120517   \n",
       "1507126  JV_120517_AdamsApple.ifc  436.2838  436.4734  ...       JV  120517   \n",
       "1507127  JV_120517_AdamsApple.ifc  436.2838  436.4734  ...       JV  120517   \n",
       "1507128  JV_120517_AdamsApple.ifc  436.2838  436.4734  ...       JV  120517   \n",
       "1507129  JV_120517_AdamsApple.ifc  436.2838  436.4734  ...       JV  120517   \n",
       "\n",
       "              title  ipa  duration  norm_duration  olddatetime    datetime  \\\n",
       "1507125  AdamsApple    ɑ    0.1896       0.125563   2017-12-05  2017-12-05   \n",
       "1507126  AdamsApple    ɑ    0.1896       0.125563   2017-12-05  2017-12-05   \n",
       "1507127  AdamsApple    ɑ    0.1896       0.125563   2017-12-05  2017-12-05   \n",
       "1507128  AdamsApple    ɑ    0.1896       0.125563   2017-12-05  2017-12-05   \n",
       "1507129  AdamsApple    ɑ    0.1896       0.125563   2017-12-05  2017-12-05   \n",
       "\n",
       "        bins ebins  \n",
       "1507125   21    29  \n",
       "1507126   21    29  \n",
       "1507127   21    29  \n",
       "1507128   21    29  \n",
       "1507129   21    29  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some sanity checks to make sure data look all right\n",
    "full_df.tail()\n",
    "# full_df[full_df.datetime=='2007-01-10']\n",
    "# full_df.loc[2950:3100,['t1_p','datetime','bins','ebins']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize vowel formants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first create smaller df that is a subset of only vowels!\n",
    "vowels = ['ɔ','ɑ','i','u','ɛ','ɪ','ʊ','ʌ','æ','ə','eɪ','aɪ','oʊ','aʊ','ɔɪ','ɚ']\n",
    "vowels_df = full_df[full_df.ipa.isin(vowels)]\n",
    "#vowels_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate formant zscores for groups defined by num_part, speaker, and vowel.\n",
    "normcols = ['speaker']\n",
    "zscorecols = ['f1', 'f2', 'f3', 'f4']\n",
    "zscore = lambda x: (x - x.mean()) / x.std()\n",
    "\n",
    "# Select columns of zscore interest, group, and calculate zscore for each group.\n",
    "zscored = vowels_df.loc[:, normcols + zscorecols].groupby(normcols).transform(zscore)\n",
    "zscored = zscored.rename(columns={'f1': 'f1_z', 'f2': 'f2_z', 'f3': 'f3_z', 'f4': 'f4_z'})\n",
    "#zscored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that observations in zscored match observations in vowels_df.\n",
    "(zscored.index == vowels_df.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine zscores with original formant measurements. (If you use .concat, only do it once!)\n",
    "# This also could be done with merge().\n",
    "vowels_df = pd.concat([vowels_df, zscored], axis=1)\n",
    "#vowels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate by-speaker means and standard deviations for each vowel\n",
    "groupcols = ['speaker' ,'ipa']\n",
    "meanvowels_df = vowels_df.groupby(groupcols).agg([np.mean, np.std])\n",
    "meanvowels_df.columns = meanvowels_df.columns.map('_'.join)\n",
    "#meanvowels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker  ipa\n",
      "GN       i      2360.868494\n",
      "         u      1788.909956\n",
      "         æ      1939.861093\n",
      "         ɑ      1516.440005\n",
      "Name: f2_mean, dtype: float64\n",
      "****************************************\n",
      "speaker  ipa\n",
      "JV       i      2282.620902\n",
      "         u      1803.614434\n",
      "         æ      1747.518762\n",
      "         ɑ      1469.379074\n",
      "Name: f2_mean, dtype: float64\n",
      "****************************************\n",
      "GN average f0 is 143.108815909\n",
      "JV average f0 is 171.36128927\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks! Compare f2 values for corner vowels for each speaker\n",
    "print(meanvowels_df.loc[('GN',['u','i','æ','ɑ']),'f2_mean'])\n",
    "print('*'*40)\n",
    "print(meanvowels_df.loc[('JV',['u','i','æ','ɑ']),'f2_mean'])\n",
    "print('*'*40)\n",
    "# Compare average f0 values for each speaker\n",
    "print('GN average f0 is', vowels_df.loc[vowels_df.speaker=='GN','f0'].mean())\n",
    "print('JV average f0 is', vowels_df.loc[vowels_df.speaker=='JV','f0'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate mean and stdev of vowel variables across videos and bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate per-video means of all variables\n",
    "video_mean = vowels_df.groupby(['datetime','speaker']).agg([np.mean, np.std])\n",
    "video_mean.columns = video_mean.columns.map('_'.join)\n",
    "# Drop bins with NaN: speaker didn't produce any tokens during that minute\n",
    "video_mean = video_mean.dropna(axis=0, how='all')\n",
    "\n",
    "video_mean_sepvowels = vowels_df.groupby(['datetime','speaker','ipa']).agg([np.mean, np.std])\n",
    "video_mean_sepvowels.columns = video_mean_sepvowels.columns.map('_'.join)\n",
    "\n",
    "bin_mean = vowels_df.groupby(['datetime','speaker','bins']).agg([np.mean, np.std])\n",
    "bin_mean.columns = bin_mean.columns.map('_'.join)\n",
    "\n",
    "equal_bin_mean = vowels_df.groupby(['datetime','speaker','ebins']).agg([np.mean, np.std])\n",
    "equal_bin_mean.columns = equal_bin_mean.columns.map('_'.join)\n",
    "\n",
    "tword_mean = vowels_df.groupby(['datetime','speaker','t1_w']).agg([np.mean, np.std])\n",
    "tword_mean.columns = tword_mean.columns.map('_'.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the same, but for only sibilants (s, z, sh, zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sibilants = ['s','ʃ','z','ʒ']\n",
    "sibilants_df = full_df[full_df.ipa.isin(sibilants)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_video_mean = sibilants_df.groupby(['datetime','speaker','ipa']).agg([np.mean, np.std])\n",
    "s_video_mean.columns = s_video_mean.columns.map('_'.join)\n",
    "\n",
    "s_bin_mean = sibilants_df.groupby(['datetime','speaker','bins','ipa']).agg([np.mean, np.std])\n",
    "s_bin_mean.columns = s_bin_mean.columns.map('_'.join)\n",
    "\n",
    "s_equal_bin_mean = sibilants_df.groupby(['datetime','speaker','ebins','ipa']).agg([np.mean, np.std])\n",
    "s_equal_bin_mean.columns = s_equal_bin_mean.columns.map('_'.join)\n",
    "\n",
    "groupcols = ['speaker' ,'ipa']\n",
    "meansibilants_df = sibilants_df.groupby(groupcols).agg([np.mean, np.std])\n",
    "meansibilants_df.columns = meansibilants_df.columns.map('_'.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export dataframes as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export vowel dfs as csv\n",
    "tword_mean.to_csv('twordmean.csv', encoding='utf-8')\n",
    "equal_bin_mean.to_csv('equalbinmean.csv', encoding='utf-8'),\n",
    "bin_mean.to_csv('binmean.csv', encoding='utf-8')\n",
    "video_mean.to_csv('videomean.csv', encoding='utf-8')\n",
    "video_mean_sepvowels.to_csv('videomean_sepvowels.csv', encoding='utf-8')\n",
    "meanvowels_df.to_csv('meanvowels.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export sibilant dfs as csv\n",
    "s_equal_bin_mean.to_csv('s_equalbinmean.csv', encoding='utf-8'),\n",
    "s_bin_mean.to_csv('s_binmean.csv', encoding='utf-8')\n",
    "s_video_mean.to_csv('s_videomean.csv', encoding='utf-8')\n",
    "meansibilants_df.to_csv('meansibilants_df.csv', encoding='utf-8')\n",
    "sibilants_df.to_csv('sibilants_df.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export largest dfs as csv -- these two take the longest, may be skipped\n",
    "vowels_df.to_csv('vowels_df.csv', encoding='utf-8')\n",
    "full_df.to_csv('full_df.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
