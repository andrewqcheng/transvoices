{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "- This notebook processes data from the TransVoices project for the purpose of analyzing acoustic change of vowels over time.\n",
    "- Two trans women YouTube vloggers' videos over a period of seven years have been transcribed as TextGrids.\n",
    "- The TextGrids were force aligned using a multi-tier version of FAVE, and formant measurements were made using ifcformant.\n",
    "- The files are ''[name].multi_align.TextGrid'' in the folder \"multi_align\" and ''[name].ifc'' in the folder \"ifc_files\".\n",
    "- The .TextGrid files are read as separate 'phone' and 'word' tiers, then merged.\n",
    "- The relevant rows from each .ifc file are extracted from the .ifc files, then concatenated to the .TextGrid dataframe.\n",
    "- Formant values are normalized by speaker and by local speech rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Initialize and read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from audiolabel import read_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant files (ifc files and textgrids) are stored here in these two directories. Make each directory an object so that files can be read from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wdifc = './ifc_files/'\n",
    "wdtg = './multi_align/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a dataframe with a column of .ifc filenames, and a dataframe with a column of .TextGrid filenames. We're reading these from wdifc and wdtg. Because the filenames contain metadata split by underscores (e.g., JV_013110_Title.ifc or .multi_align.TextGrid), we'll use regular expressions to extract this data and put it into new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ifc_list = pd.DataFrame(os.listdir(wdifc), columns=['ifcname'])\n",
    "codeifc = ifc_list.ifcname.str.extract(r'^(?P<speaker>.+)_(?P<date>\\d+)_(?P<title>.+)\\.ifc$',expand=True)\n",
    "ifc_list = pd.concat([ifc_list, codeifc], axis=1) # axis=1 concatenates columns\n",
    "\n",
    "tg_list = pd.DataFrame(os.listdir(wdtg), columns=['tgname'])\n",
    "codetg = tg_list.tgname.str.extract(r'^(?P<speaker>.+)_(?P<date>\\d+)_(?P<title>.+)\\.multi_align\\.TextGrid$',expand=True)\n",
    "tg_list = pd.concat([tg_list, codetg], axis=1) # axis=1 concatenates columns\n",
    "\n",
    "# Sanity check\n",
    "#tgdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes there won't be the same number of .ifc and .TextGrid files. We'll merge the two list dataframes using pd.merge, and the final product is matchdf, showing which .ifc files have a .TextGrid match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code = ['speaker','date','title']\n",
    "matchdf = pd.merge(ifc_list, tg_list, on=code, how='inner')\n",
    "\n",
    "# Sanity check\n",
    "count = matchdf.shape[0]\n",
    "print(\"matchdf has\",count,\"matched textgrids and ifc files.\")\n",
    "matchdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function defined below is used to find the 'previous' and 'following' phone for any given phone, based on the values in the TextGrid's phone tier that come just above or below it. At the beginnings and ends of a column, it will fill in the space with 'val'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_1d_with_constant(a, shift, val):\n",
    "    '''\n",
    "    Roll a list of values 'a' by amount 'shift' in a way similar to np.roll(),\n",
    "    but instead of wrapping values, replace wrapped elements with a constant 'val'\n",
    "    '''\n",
    "    if shift >= 0:\n",
    "        index = np.arange(len(a))\n",
    "    else:\n",
    "        index = np.arange(len(a) * -1, 0, 1)\n",
    "    return np.pad(a, np.abs(shift), 'constant', constant_values=val)[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting ready to read in all the labels from the phone and word tiers of the actual TextGrids. As soon as we read in the labels, we're going to add some new columns. One of them will be 'vcontext', a vowel that is specific to a phonological context; so we define certain phonological classes as lists below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasals = ['N','M']\n",
    "lateral = ['L']\n",
    "coronal = ['T','D']\n",
    "lowfront = ['AE','AE1','AE2','AE0'] # to separate AE from AEN\n",
    "hiback = ['UW','UW1','UW2','UW0'] # to separate UW from pre-lateral UWL\n",
    "midback = ['OW','OW1','OW2','OW0'] # to separate OW from pre-lateral OWL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use audiolabel's read_label function to compile a dataframe from all of the .TextGrid files. This loops over each aligned TextGrid in matchdf and extracts the labels from 'phone' tier and 'word' tier. The read_label function adds a column called 'fname' that reports what file all the labels came from.\n",
    "\n",
    "It also creates the 'prev' and 'foll' phone columns using roll_1d_with_constant; a 'vowel' and 'stress' column using regular expressions and the 'phone' column; an 'ipa' column using a conversion table of ARPABET and IPA symbols; and the 'vcontext' column mentioned previously.\n",
    "\n",
    "The products are two dataframes, phonedf and worddf, that contain all the phones/words in all the TextGrids in matchdf, and they will be merged next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phones = []\n",
    "words = []\n",
    "ph2ipa = pd.read_table('arpabet2ipa.txt', names=('phone','ipa')) # Read in conversion table for ARPA to ipa\n",
    "\n",
    "for tgname in matchdf.tgname: # For debugging, replace matchdf.tgname with matchdf.head(2).tgname\n",
    "    [phdf, wddf] = read_label(os.path.join(wdtg, tgname), 'praat', addcols=['barename'], tiers=['phone','word'])\n",
    "    phdf=phdf.assign(prev=roll_1d_with_constant(phdf.phone,1,'sp'),\n",
    "                     foll=roll_1d_with_constant(phdf.phone,-1,'sp'),\n",
    "                     # Create two new columns, 'vowel' and 'stress', based on regular expressions in 'phone' column\n",
    "                     vowel=phdf.phone.str.extract(r'^(?P<vowel>.+)(?P<stress>\\d+)$', expand=True).iloc[:,0],\n",
    "                     stress=phdf.phone.str.extract(r'^(?P<vowel>.+)(?P<stress>\\d+)$', expand=True).iloc[:,1],\n",
    "                    )\n",
    "    phdf=phdf.merge(ph2ipa,how='left',on='phone') # merge ipa df with phones\n",
    "    phdf=phdf.assign(vcontext=phdf.vowel)\n",
    "    phdf.loc[(phdf.phone.isin(lowfront)) & (phdf.foll.isin(nasals)),'vcontext'] = 'AEN'\n",
    "    phdf.loc[(phdf.phone.isin(hiback)) & (phdf.foll.isin(lateral)),'vcontext'] = 'UWL'\n",
    "    phdf.loc[(phdf.phone.isin(midback)) & (phdf.foll.isin(lateral)),'vcontext'] = 'OWL'\n",
    "    phones.append(phdf)\n",
    "    words.append(wddf)\n",
    "phonedf = pd.concat(phones)\n",
    "worddf = pd.concat(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "#phonedf['stress'].count() # this is how many syllables are in the entire phonedf\n",
    "#phonedf.head(20)\n",
    "#worddf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Get local speech rate from textgrid\n",
    "\n",
    "Thanks to Geoff Bacon for this section. The first function, make_rows_stress_count, gets local speech rate in syllables, defined as stressed vowels per 20 TextGrid rows. Twenty rows is arbitrary, and can give you about seven words (twenty phones/segments, plus pauses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rows_stress_count(df, window_size):\n",
    "    \"\"\"\n",
    "    Returns `df` with an extra column of the moving count of the stress column.\n",
    "    `df` must have a column called `stress` and `fname` (identifying each video, for groupby)\n",
    "    Counts all values in `stress` as syllables in `binary_stress`.\n",
    "    Uses `.rolling` to count `binary_stress` values in a window size of X rows.\n",
    "    Fills in edge cases (begininng and end of window) using `bfill` and `ffill`.\n",
    "    \"\"\"\n",
    "    df['binary_stress'] = df['stress'].notnull()\n",
    "    df['binary_stress'].replace({False: 0, True: 1}, inplace=True)\n",
    "    grouped = df.groupby('fname')\n",
    "    rolling_count = grouped['binary_stress'].rolling(window_size, center=True).sum()\n",
    "    ### moved ['binary_stress'] to after grouped instead of after .sum() to fix an error ###\n",
    "    rolling_count = rolling_count.groupby(level='fname').fillna(method='bfill')\n",
    "    rolling_count = rolling_count.groupby(level='fname').fillna(method='ffill')\n",
    "    multiindexed_df = df.set_index('fname', append=True).swaplevel()\n",
    "    renamed_rolling_count = rolling_count.rename('rows_stress_count').to_frame()\n",
    "    merged = pd.merge(multiindexed_df, renamed_rolling_count, left_index=True, right_index=True)\n",
    "    df_with_rolling_count = merged.reset_index().drop('level_1', axis=1)\n",
    "    return df_with_rolling_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to phonedf with a window size of 20 rows\n",
    "df = make_rows_stress_count(phonedf, 20)\n",
    "phonedf = df\n",
    "\n",
    "# Sanity checks\n",
    "#phonedf.head() # df should have columns \"binary_stress\" and \"rows_stress_count\"\n",
    "#print(phonedf.binary_stress.unique()) # should get [0 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second and third functions, process and make_temporal_rolling_stress_count, get local speech rate in syllables, defined as stressed vowels per 30 seconds in TextGrid. Using actual time requires reading from the 't1' column of phonedf but is more reliable for speech rate than using dataframe rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def process(dataframe, offset):\n",
    "    \"\"\"\n",
    "    Changes `t1` in dataframe to seconds units, counts up all values in `binary_stress` every 30 seconds\n",
    "    \"\"\"\n",
    "    dataframe['t1_as_datetime'] = pd.to_datetime(dataframe['t1'], unit='s')\n",
    "    dataframe['time_stress_count'] = dataframe.rolling('30s', on='t1_as_datetime')['binary_stress'].sum()\n",
    "    start = dataframe['t1_as_datetime'].iloc[0]\n",
    "    dataframe['offset'] = dataframe['t1_as_datetime'] - start\n",
    "    dataframe['beginning'] = dataframe['offset'] < offset\n",
    "    dataframe.loc[dataframe['beginning'], 'time_stress_count'] = np.NaN\n",
    "    dataframe['time_stress_count'].fillna(method='bfill', inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_temporal_rolling_stress_count(df, time_in_seconds):\n",
    "    \"\"\"\n",
    "    Returns `df` with an extra column of the moving count of the stress column of length `time_in_seconds`.\n",
    "    `df` must have a column called `stress` and `fname`.\n",
    "    Uses `.rolling` to count `binary_stress` values per window of 30 seconds.\n",
    "    \"\"\"\n",
    "    offset = timedelta(seconds=time_in_seconds)\n",
    "    \n",
    "    fnames = phonedf['fname'].unique()\n",
    "    result = []\n",
    "    for fname in fnames:\n",
    "        tmp = phonedf[phonedf['fname'] == fname]\n",
    "        tmp = process(tmp, offset)\n",
    "        result.append(tmp)\n",
    "    df = pd.concat(result, ignore_index=True)\n",
    "    return df.drop(['offset', 't1_as_datetime', 'beginning'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to phonedf with a window size of 30 seconds\n",
    "df = make_temporal_rolling_stress_count(phonedf, 30)\n",
    "phonedf = df\n",
    "\n",
    "# Sanity check\n",
    "phonedf.head() # should now have final column 'time_stress_count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Merge phone df & word df to create full textgrid df\n",
    "\n",
    "Now we have all the TextGrid information we need. We will merge the phone dataframe with the word dataframe to create fulltgdf, or the dataframe that includes phone and word information. We'll merge the two dataframes by their 't1', since the start time of a word is the same no matter which phone of that word you're analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worddf = worddf.assign(t1_w=worddf.t1) # Duplicate the 't1' column and assign it to 't1_w'\n",
    "\n",
    "def mergepw(p,w):\n",
    "    b=p['barename'].values[0] # find barename value of argument p\n",
    "    locw=w.loc[w.barename==b,['t1','t1_w','t2','word']] # select words that have barename value 'b'\n",
    "    return pd.merge_asof(p,locw,on='t1',suffixes=['_p','_w']) # merge, create suffixes for duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the function to merge phone and word textgrids and create the full textgrid dataframe\n",
    "fulltgdf = phonedf.groupby('barename').apply(mergepw,w=worddf)\n",
    "\n",
    "# Reset multilevel index, ditch .multi_align extension, rename time column, drop fname\n",
    "fulltgdf = fulltgdf.reset_index(level='barename',drop=True)\n",
    "fulltgdf = fulltgdf.assign(barename=fulltgdf.barename.str.replace('.multi_align',''))\n",
    "fulltgdf = fulltgdf.rename(columns={'t1':'t1_p'}).drop(columns='fname')\n",
    "\n",
    "# replace all empty 'phone' cells with NaN, then drop those rows\n",
    "fulltgdf['phone'].replace('', np.nan, inplace=True)\n",
    "fulltgdf.dropna(subset=['phone'], inplace=True)\n",
    "\n",
    "# Sanity checks\n",
    "#print(fulltgdf.columns)\n",
    "#fulltgdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to subset fulltgdf so that it contains only vowels. To work on consonants, just change what is subsetted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = ['ɔ','ɑ','i','u','ɛ','ɪ','ʊ','ʌ','æ','ə','eɪ','aɪ','oʊ','aʊ','ɔɪ','ɚ']\n",
    "vowels_df = fulltgdf[fulltgdf.ipa.isin(vowels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Merge fulltgdf with ifc files\n",
    "\n",
    "Now we will read in the .ifc files. But we will only read in certain rows of each .ifc file, to prevent our final dataframe from being too large. The functions defined below will figure out which row of each .ifc file to read in, based on which value of 'sec' is closest to the 25%, 50%, and 75% timepoint in the duration of each vowel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_nearest_time(df, t, timecol):\n",
    "    '''Get the row nearest in time to specified value.\n",
    "    In case of tie, first row is returned.\n",
    "    ***NOTE***\n",
    "    df[timecol] must be sorted!!!\n",
    "    '''\n",
    "    return df.loc[(df[timecol] - t).abs().idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timepoints(t,ifc):\n",
    "    dur = t.t2_p-t.t1_p # Calculate the duration of the vowel\n",
    "    # Create series; one for the 25% values, one for the 50% values, and one for the 75% values\n",
    "    idx25 = get_row_nearest_time(ifc,(t.t1_p+(0.25*dur)),'sec') \\\n",
    "        .rename({'f1':'f1_25','f2':'f2_25','f3':'f3_25','f0':'f0_25'})\n",
    "    idx50 = get_row_nearest_time(ifc,(t.t1_p+(0.50*dur)),'sec') \\\n",
    "        .rename({'f1':'f1_50','f2':'f2_50','f3':'f3_50','f0':'f0_50'})\n",
    "    idx75 = get_row_nearest_time(ifc,(t.t1_p+(0.75*dur)),'sec') \\\n",
    "        .rename({'f1':'f1_75','f2':'f2_75','f3':'f3_75','f0':'f0_75'})\n",
    "    # Concatenate the series-converted-to-dataframes and drop extra columns\n",
    "    return pd.concat(\n",
    "        [\n",
    "            pd.DataFrame.from_records([t]),\n",
    "            pd.DataFrame.from_records([idx25]),\n",
    "            pd.DataFrame.from_records([idx50]).drop(['sec','rms'],axis='columns'),\n",
    "            pd.DataFrame.from_records([idx75]).drop(['sec','rms'],axis='columns')\n",
    "        ],\n",
    "        axis='columns'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the function below will open up the .ifc file using read_table, read the columns defined in usecols, and apply the function get_timepoints to read only the desired rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeifc(df,wdifc):\n",
    "    usecols = ['sec','rms','f0', 'f1', 'f2', 'f3']\n",
    "    ifcname = df.barename.iloc[0]+'.ifc'\n",
    "    ifc = pd.read_table(os.path.join(wdifc,ifcname),usecols=usecols)\n",
    "    vowelsdf = pd.concat(\n",
    "        df.apply(get_timepoints,axis=1,ifc=ifc) \\\n",
    "        .values.tolist() # concatenates the list of dataframes into a dataframe\n",
    "    )\n",
    "    return vowelsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the function to vowels_df, but group it by video (barename). The cell below took about 10 minutes to run for 50 .ifc files (~72MB of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = vowels_df.groupby('barename').apply(mergeifc,wdifc=wdifc) # This will take some time! Go make some tea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHECK POINT ###\n",
    "# This cell saves merged_df as a .csv; to save time during future debugging, just read in mergeddf.csv\n",
    "\n",
    "merged_df.to_csv('mergeddf.csv', encoding='utf-8')\n",
    "merged_df = pd.read_csv('mergeddf.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = merged_df.reset_index(drop=True) # reset hierarchical index\n",
    "\n",
    "# Sanity check\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Clean up full_df\n",
    "\n",
    "The dataframe full_df now contains every vowel from every video that has been analyzed. Each vowel has measurements for f0, F1, F2, and F3 at the 25/50/75% timepoints, as well as meta information about the word, previous/following segments, etc. The last few lines will clean up the dataframe a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we already calculated vowel duration, we'll do again here using t1_p and t2_p, and also provide vowel duration normalized by the number of syllables in the phone's 30-second window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_df['duration'] = full_df['t2_p'] - full_df['t1_p']\n",
    "full_df['norm_duration'] = full_df['duration']/full_df['time_stress_count']*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll use regular expressions to add the metadata columns for speaker, date, and time, all taken from the values in the column 'barename'. We'll have to use pd.to_datetime to change the values in 'date' from the monthdayyear format to something more useable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdt = full_df.barename.str.extract(r'^(?P<speaker>.+)_(?P<date>\\d+)_(?P<title>.+)', expand=True)\n",
    "full_df = pd.concat([full_df, sdt], axis=1)\n",
    "\n",
    "# Convert dates to useable formats\n",
    "full_df['olddatetime'] = pd.to_datetime(full_df['date'], format='%m%d%y', errors='coerce') # datetime format\n",
    "full_df['datetime'] = full_df.olddatetime.astype(str).str.strip() # convert to string\n",
    "\n",
    "# Sanity check\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Section 6: Bin videos by time\n",
    "\n",
    "In this section, we want to see if there are any formant patterns based on point of time within a video. We'll bin videos in two ways. First, 'bins' that are set at 20 seconds long, so that each video can have between 1 and 31 bins (31x20=620 seconds, and the longest video in the corpus is less than 10 minutes long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create up to 31 bins of 20 seconds each for each video, grouping by 'barename'\n",
    "binseq = full_df.groupby('barename').apply(\n",
    "            lambda y: np.array([0,20,40,60,80,100,120,140,160,180,200,220,240,260,280,300,\n",
    "                  320,340,360,380,400,420,440,460,480,500,520,540,560,580,600,620]\n",
    "                               + y.t1_p.min() - 0.01)) # make bins of 20 secs each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, 'ebins' that splits every video into 30 bins of equal length. The bin size will depend on the length of the video, such that a 300-second (5-minute) video will have 30 bins of 10 seconds each, and a 120-second (2-minute) video will have 30 bins of 4 seconds each, etc. The cell below creates 'ebins' and 'bins'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create  a dataframe that cuts full_df into 30 bins for each video, grouping by 'datetime'\n",
    "ebins = pd.DataFrame(full_df.groupby('barename').apply(\n",
    "    lambda q: pd.cut(q['t1_p'], bins=30, labels=False)))\n",
    "ebins = ebins.rename(columns={'t1_p':'binidx'}) #ebins should have 30 bins per video\n",
    "\n",
    "# Create a dataframe that cuts full_df into 20sec bins for each video, grouping by 'datetime'\n",
    "bins = pd.DataFrame(full_df.groupby('barename').apply(\n",
    "    lambda r: pd.cut(r['t1_p'], bins=binseq.loc[r.name], labels=False)))\n",
    "bins = bins.rename(columns={'t1_p':'binidx'}) #binseq should have variable bins per video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we've used .groupby, we'll have to reset the indexes (removing 'dt'), and then we'll assign 'bins' and 'ebins' to full_df as new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = bins.reset_index(level=0)\n",
    "ebins = ebins.reset_index(level=0)\n",
    "\n",
    "full_df = full_df.assign(bins=bins.binidx)\n",
    "full_df = full_df.assign(ebins=ebins.binidx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Normalize vowel formants\n",
    "The stats function from scipy includes an automatic z-score computer that 'stacks' all the selected columns and scores them together. This way, f0 at all three timepoints (25, 50, and 75%) are scored together rather than separately, even though they are in separate columns. We define a function that creates three new dataframes for f0, F1, and F2 z-scores, then concatenates them to the input dataframe (df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def myzscore(df):\n",
    "    z0 = pd.DataFrame(\n",
    "        stats.zscore(\n",
    "            df.loc[:,['f0_25','f0_50','f0_75']],\n",
    "            axis=None),\n",
    "    columns = ['f0_25_z','f0_50_z','f0_75_z'],\n",
    "        index = df.index\n",
    "    )\n",
    "    z1 = pd.DataFrame(\n",
    "        stats.zscore(\n",
    "            df.loc[:,['f1_25','f1_50','f1_75']],\n",
    "            axis=None),\n",
    "    columns = ['f1_25_z','f1_50_z','f1_75_z'],\n",
    "        index = df.index\n",
    "    )\n",
    "    z2 = pd.DataFrame(\n",
    "        stats.zscore(\n",
    "            df.loc[:,['f2_25','f2_50','f2_75']],\n",
    "            axis=None),\n",
    "    columns = ['f2_25_z','f2_50_z','f2_75_z'],\n",
    "        index = df.index\n",
    "    )\n",
    "    df = pd.concat([df,z0,z1,z2],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function to full_df, grouped by speaker. Only run this once, as .concat will throw an error if you run it multiple times. Also, .groupby creates a hierarchical index that will be dropped in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.groupby('speaker').apply(myzscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.reset_index(level=0,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8: Calculate by-speaker and by-video means, standard deviations\n",
    "Group full_df by speaker and vowel (either by phoneme ('ipa') or by phoneme in phonological context ('vcontext') and apply calculations of mean and standard deviation to every numeric column. This will produce extraneous columns, such as mean of 't1_p'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate by-speaker means and standard deviations for each vowel\n",
    "meanvowels_df = full_df.groupby(['speaker','ipa']).agg([np.mean,np.std])\n",
    "meanvowels_df.columns = meanvowels_df.columns.map('_'.join)\n",
    "#meanvowels_df.head()\n",
    "\n",
    "# Same, but by vcontext (to separate AE from AEN, etc.)\n",
    "meanvowels_context_df = full_df.groupby(['speaker','vcontext']).agg([np.mean,np.std])\n",
    "meanvowels_context_df.columns = meanvowels_context_df.columns.map('_'.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, the cell below calculates F2 and f0 values for some vowels for each speaker. Check to see if these all look reasonable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(meanvowels_context_df.loc[('GN',['UW','UWL','IY','AE','AEN','AA']),'f2_50_mean'])\n",
    "print('*'*40)\n",
    "print(meanvowels_context_df.loc[('JV',['UW','UWL','IY','AE','AEN','AA']),'f2_50_mean'])\n",
    "print('*'*40)\n",
    "print('GN average AE F2 at midpoint is', meanvowels_context_df.loc[('GN','AE'),'f2_50_mean'],\n",
    "      'while average AEN F2 at midpoint is', meanvowels_context_df.loc[('GN','AEN'),'f2_50_mean'])\n",
    "print('*'*40)\n",
    "\n",
    "print('GN average f0 is', full_df.loc[vowels_df.speaker=='GN','f0_50'].mean())\n",
    "print('JV average f0 is', full_df.loc[vowels_df.speaker=='JV','f0_50'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a few \"summary\" dataframes from mean/standard deviation calculations on different groupings of the data in vowels_df. All of these will be exported, along with vowels_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate per-video means of all variables\n",
    "video_mean = full_df.groupby(['datetime','speaker']).agg([np.mean, np.std])\n",
    "video_mean.columns = video_mean.columns.map('_'.join)\n",
    "# Drop bins with NaN: speaker didn't produce any tokens during that minute\n",
    "video_mean = video_mean.dropna(axis=0, how='all')\n",
    "\n",
    "video_mean_sepvowels = full_df.groupby(['datetime','speaker','ipa']).agg([np.mean, np.std])\n",
    "video_mean_sepvowels.columns = video_mean_sepvowels.columns.map('_'.join)\n",
    "\n",
    "video_mean_contextvowels = full_df.groupby(['datetime','speaker','vcontext']).agg([np.mean, np.std])\n",
    "video_mean_contextvowels.columns = video_mean_contextvowels.columns.map('_'.join)\n",
    "\n",
    "bin_mean = full_df.groupby(['datetime','speaker','bins']).agg([np.mean, np.std])\n",
    "bin_mean.columns = bin_mean.columns.map('_'.join)\n",
    "\n",
    "equal_bin_mean = full_df.groupby(['datetime','speaker','ebins']).agg([np.mean, np.std])\n",
    "equal_bin_mean.columns = equal_bin_mean.columns.map('_'.join)\n",
    "\n",
    "tword_mean = full_df.groupby(['datetime','speaker','t1_w']).agg([np.mean, np.std])\n",
    "tword_mean.columns = tword_mean.columns.map('_'.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9: Export dataframes as csv files\n",
    "UTF-8 encoding is important due to the use of IPA symbols in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tword_mean.to_csv('twordmean.csv', encoding='utf-8')\n",
    "equal_bin_mean.to_csv('equalbinmean.csv', encoding='utf-8')\n",
    "bin_mean.to_csv('binmean.csv', encoding='utf-8')\n",
    "video_mean.to_csv('videomean.csv', encoding='utf-8')\n",
    "video_mean_sepvowels.to_csv('videomean_sepvowels.csv', encoding='utf-8')\n",
    "video_mean_contextvowels.to_csv('videomean_convowels.csv', encoding='utf-8')\n",
    "meanvowels_df.to_csv('meanvowels.csv', encoding='utf-8')\n",
    "meanvowels_context_df.to_csv('meanvowelscontext.csv', encoding='utf-8')\n",
    "full_df.to_csv('full_df.csv', encoding='utf-8')\n",
    "full_df.to_csv('vowels_df.csv', encoding='utf-8') # full_df.csv and vowels_df.csv will be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS SECTION IS NO LONGER NECESSARY ###\n",
    "### get_timepoints function defined earlier already does all of this ###\n",
    "\n",
    "# def get_timepoints(df):\n",
    "#     ### To do later: Assert that all values in 'duration' are the same in df\n",
    "#     vt50 = (df.duration.iloc[0] * 0.50) + df.t1_p.iloc[0]\n",
    "#     vt25 = (df.duration.iloc[0] * 0.25) + df.t1_p.iloc[0]\n",
    "#     vt75 = (df.duration.iloc[0] * 0.75) + df.t1_p.iloc[0]\n",
    "#     # Get abs values of times - 25/50/75% times, choose index of minimum value (closest row) w/ argmin\n",
    "#     vidx50 = df.iloc[(df.sec - vt50).abs().values.argmin()]\n",
    "#     vidx25 = df.iloc[(df.sec - vt25).abs().values.argmin()] # If 2 minima, takes 1st idx/row\n",
    "#     vidx75 = df.iloc[(df.sec - vt75).abs().values.argmin()] # If 2 minima, takes 1st idx/row\n",
    "#     # Create a df from dictionary, keys from vidx.column name\n",
    "#     dictionary = {'barename':df.barename.iloc[0],'speaker':df.speaker.iloc[0],\n",
    "#                  'phone':df.phone.iloc[0],'ipa':df.ipa.iloc[0],'vcontext':df.vcontext.iloc[0],\n",
    "#                  't1_p':df.t1_p.iloc[0],'t2_p':df.t2_p.iloc[0],\n",
    "#                  'duration':df.duration.iloc[0],'norm_duration':df.norm_duration.iloc[0],\n",
    "#                  'foll':df.foll.iloc[0],'prev':df.prev.iloc[0],\n",
    "#                  'stress':df.stress.iloc[0],'rolling_stress_count':df.rolling_stress_count.iloc[0],\n",
    "#                  'rolling_count':df.rolling_count.iloc[0],\n",
    "#                  'word':df.word.iloc[0],'t1_w':df.t1_w.iloc[0],'t2_w':df.t2_w.iloc[0],\n",
    "#                  'f1idx25':vidx25.f1,'f1idx50':vidx50.f1,'f1idx75':vidx75.f1,\n",
    "#                  'f2idx25':vidx25.f2,'f2idx50':vidx50.f2,'f2idx75':vidx75.f2,\n",
    "#                  'f0idx25':vidx25.f0,'f0idx50':vidx50.f0,'f0idx75':vidx75.f0,\n",
    "#                  'f1idx25_z':vidx25.f1_z,'f1idx50_z':vidx50.f1_z,'f1idx75_z':vidx75.f1_z,\n",
    "#                  'f2idx25_z':vidx25.f2_z,'f2idx50_z':vidx50.f2_z,'f2idx75_z':vidx75.f2_z,\n",
    "#                  'f0idx25_z':vidx25.f0_z,'f0idx50_z':vidx50.f0_z,'f0idx75_z':vidx75.f0_z,\n",
    "#                  'olddatetime':df.olddatetime.iloc[0],'datetime':df.datetime.iloc[0],\n",
    "#                  'bins':df.bins.iloc[0],'ebins':df.ebins.iloc[0]}\n",
    "#     newdf = pd.DataFrame([dictionary])\n",
    "#     return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS SECTION IS NO LONGER NECESSARY ###\n",
    "### z-scoring using scipy in section 7 ###\n",
    "\n",
    "# # Calculate formant zscores for groups defined by num_part, speaker, and vowel.\n",
    "# normcols = ['speaker']\n",
    "# zscorecols = ['f1', 'f2', 'f0']\n",
    "# zscore = lambda x: (x - x.mean()) / x.std()\n",
    "\n",
    "# # Select columns of zscore interest, group, and calculate zscore for each group.\n",
    "# zdf = full_df.loc[:, normcols + zscorecols].groupby(normcols).transform(zscore)\n",
    "# zdf = zdf.rename(columns={'f1': 'f1_z', 'f2': 'f2_z', 'f0': 'f0_z'})\n",
    "\n",
    "# # Verify that observations in zscored match observations in vowels_df.\n",
    "# (zdf.index == full_df.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine zscores with original formant measurements. (If you use .concat, only do it once!)\n",
    "# full_df = pd.concat([v_df, zdf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS SECTION IS NO LONGER NECCESSARY ###\n",
    "### We've moved sibilants to a different notebook. ###\n",
    "\n",
    "# sibilants = ['s','ʃ','z','ʒ']\n",
    "# sibilants_df = full_df[full_df.ipa.isin(sibilants)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_video_mean = sibilants_df.groupby(['datetime','speaker','ipa']).agg([np.mean, np.std])\n",
    "# s_video_mean.columns = s_video_mean.columns.map('_'.join)\n",
    "\n",
    "# s_bin_mean = sibilants_df.groupby(['datetime','speaker','bins','ipa']).agg([np.mean, np.std])\n",
    "# s_bin_mean.columns = s_bin_mean.columns.map('_'.join)\n",
    "\n",
    "# s_equal_bin_mean = sibilants_df.groupby(['datetime','speaker','ebins','ipa']).agg([np.mean, np.std])\n",
    "# s_equal_bin_mean.columns = s_equal_bin_mean.columns.map('_'.join)\n",
    "\n",
    "# groupcols = ['speaker' ,'ipa']\n",
    "# meansibilants_df = sibilants_df.groupby(groupcols).agg([np.mean, np.std])\n",
    "# meansibilants_df.columns = meansibilants_df.columns.map('_'.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export sibilant dfs as csv\n",
    "# s_equal_bin_mean.to_csv('s_equalbinmean.csv', encoding='utf-8'),\n",
    "# s_bin_mean.to_csv('s_binmean.csv', encoding='utf-8')\n",
    "# s_video_mean.to_csv('s_videomean.csv', encoding='utf-8')\n",
    "# meansibilants_df.to_csv('meansibilants_df.csv', encoding='utf-8')\n",
    "# sibilants_df.to_csv('sibilants_df.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### THIS SECTION IS NO LONGER NECESSARY ###\n",
    "### But if you ever want to merge the entire ifc file with the entire tg files ###\n",
    "### you should use this. Keeping for future reference. ###\n",
    "\n",
    "# This reads in all of the ifc files in their entirety\n",
    "# Columns to read from .ifc files\n",
    "#usecols = ['sec','rms','f0', 'f1', 'f2', 'f3', 'f4']\n",
    "# Compile a dataframe from all of the .ifc files.\n",
    "#dfs = []\n",
    "#counter = 0\n",
    "#for ifcname in matchdf.ifcname:\n",
    "#    if counter == 100: # For debugging, run only two iterations.\n",
    "#        break\n",
    "#    df = pd.read_table(os.path.join(wdifc, ifcname), usecols=usecols)\n",
    "#    df = df.assign(ifcname=ifcname)\n",
    "#    dfs.append(df)\n",
    "#    counter += 1\n",
    "#fullifcdf = pd.concat(dfs)\n",
    "#fullifcdf = fullifcdf.assign(barename = fullifcdf.ifcname.str.replace('.ifc','')) # take out the extension\n",
    "\n",
    "# Sanity check\n",
    "#fullifcdf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read column from each .ifc file and merge on t1 with corresponding .textgrid file\n",
    "#def mergeit(i, pw):\n",
    "#    bn = i['barename'].values[0] # find barename value of argument 'i', assign it to 'b'\n",
    "#    locb = pw.loc[pw.barename==bn,:] # select the words that have barename value 'b'\n",
    "#    return pd.merge_asof(i, locb, left_on='sec', right_on='t1_p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by barename, apply the merge function\n",
    "#full_df = fullifcdf.groupby('barename').apply(mergeit, pw=fulltgdf)\n",
    "#full_df = full_df.drop('barename_x',axis=1).rename(columns={'barename_y':'barename'}) # how to prevent _x/_xy???\n",
    "\n",
    "# Sanity check\n",
    "#full_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
